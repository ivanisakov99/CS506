{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2 (100 Points)\n",
    "\n",
    "The goal of this homework is to get more practice with pandas and get practice with clustering on various datasets.\n",
    "\n",
    "\n",
    "\n",
    "## Exercise 1 - (50 points)\n",
    "\n",
    "This exercise will be using the [Airbnb dataset](http://insideairbnb.com/get-the-data.html) for NYC called `listings.csv`. You can download it directly [here](http://data.insideairbnb.com/united-states/ny/new-york-city/2021-11-02/visualisations/listings.csv)\n",
    "\n",
    "a) Produce a Heatmap using the Folium package (you can install it using pip) of the mean listing price per location (lattitude and longitude) over the NYC map. (5 points)\n",
    "\n",
    "Hints:\n",
    "1. generate a base map of NYC to plot over: default_location=[40.693943, -73.985880] \n",
    "2. generate an HTML file named `index.html` - open it in your browser and you'll see the heatmap \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "import pandas as pd\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(dataframe: pd.DataFrame, weight='price', zoom= 11):\n",
    "    prices = defaultdict(list)\n",
    "    \n",
    "    for row in dataframe.iterrows():\n",
    "        prices[\n",
    "            (row[1]['latitude'], row[1]['longitude'])\n",
    "        ].append(row[1][weight])\n",
    "\n",
    "    temp_map = folium.Map(\n",
    "        location=[40.693943, -73.985880],\n",
    "        zoom_start=zoom,\n",
    "    )\n",
    "\n",
    "    heat_data = [[key[0], key[1], sum(val) / len(val)]\n",
    "             for key, val in prices.items()]\n",
    "\n",
    "    HeatMap(heat_data).add_to(temp_map)\n",
    "\n",
    "    return temp_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('listings.csv', dtype=object)\n",
    "df['price'] = df['price'].astype(int)\n",
    "\n",
    "m = display(df)\n",
    "m.save('1a.html')\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Normalize the price by subtracting the mean and dividing by the standard deviation. Then reproduce the heatmap from a). Comment on any differences you observe. - (5 points )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df.copy(deep=True)\n",
    "temp['price'] = temp['price'].subtract(temp['price'].mean()).divide(temp['price'].std())\n",
    "\n",
    "m = display(temp)\n",
    "m.save('1b.html')\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> The heatmap seems to be more accurate, due to the normalisation of the prices. So the more expensive prices do not distort the less expensive ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Normalize the original price using sklearn's [MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) to the interval [0,1]. Then reproduce the Heatmap from a). Comment on any differences you observe.  - (5 points)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "temp = df.copy(deep=True)\n",
    "temp[['price']] = min_max_scaler.fit_transform(temp[['price']])\n",
    "\n",
    "m = display(temp)\n",
    "m.save('1c.html')\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> It seems to be similar to the heatmap produced in b), since we are squashing all of the values to fit between 0 and 1 but still keeping the original spread of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Plot a bar chart of the average price (un-normalized) per room type. Briefly comment on the relation between price and room type. - (2.5 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "mean_price_per_room_type = defaultdict(list)\n",
    "\n",
    "for row in df.iterrows():\n",
    "    mean_price_per_room_type[row[1]['room_type']].append(row[1]['price'])\n",
    "\n",
    "for key, val in mean_price_per_room_type.items():\n",
    "    mean_price_per_room_type[key] = sum(val) / len(val)\n",
    "\n",
    "plt.bar(mean_price_per_room_type.keys(), mean_price_per_room_type.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> The hotel rooms seems to be the most expensive ones, then followed by the whole house/flat. Counterintuitively, the private rooms are cheaper than shared rooms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Plot on the NYC map the top 10 most expensive listings - (2.5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp: pd.DataFrame = df.copy(deep=True)\n",
    "temp = temp.sort_values(by=['price'], ascending=False).drop_duplicates(\n",
    "    subset=['latitude', 'longitude'])\n",
    "\n",
    "temp = temp[:][:10]\n",
    "m = display(temp, zoom=12)\n",
    "m.save('1e.html')\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) Plot on the NYC map the top 10 most reviewed listings - (2.5 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['number_of_reviews'] = df['number_of_reviews'].astype(int)\n",
    "temp = df.copy(deep=True)\n",
    "temp = temp.sort_values(by=['number_of_reviews'], ascending=False)\n",
    "\n",
    "temp = temp[:][:10]\n",
    "m = display(temp, 'number_of_reviews', zoom=12)\n",
    "m.save('1f.html')\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g) Plot on the NYC map the top 10 most available listings - (2.5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['availability_365'] = df['availability_365'].astype(int)\n",
    "temp = df.copy(deep=True)\n",
    "temp = temp.sort_values(by=['availability_365'], ascending=False)\n",
    "\n",
    "temp = temp[:][:10]\n",
    "m = display(temp, 'availability_365')\n",
    "m.save('1g.html')\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h) Using `longitude`, `latitude`, `price`, and `number_of_reviews`, use Kmeans to create 5 clusters. Plot the points on the NYC map in a color corresponding to their cluster. - (5 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "import numpy as np\n",
    "\n",
    "temp = df.copy(deep=True)\n",
    "temp = temp[['longitude', 'latitude', 'price', 'number_of_reviews']]\n",
    "\n",
    "n_clusters = 5\n",
    "kmeans = KMeans(n_clusters=n_clusters).fit(temp)\n",
    "temp['cluster'] = kmeans.labels_\n",
    "\n",
    "colors_array = cm.rainbow(np.linspace(0, 1, n_clusters))\n",
    "rainbow = [colors.rgb2hex(i) for i in colors_array]\n",
    "m = folium.Map(\n",
    "    location=[40.693943, -73.985880],\n",
    "    zoom_start=11,\n",
    ")\n",
    "\n",
    "for lat, long, cluster in zip(temp['latitude'], temp['longitude'], temp['cluster']):\n",
    "    folium.vector_layers.CircleMarker(\n",
    "        [lat, long],\n",
    "        radius=1,\n",
    "        fill=True,\n",
    "        color=rainbow[cluster - 1],\n",
    "        fill_color=rainbow[cluster - 1],\n",
    "        fill_opacity=0.9,\n",
    "    ).add_to(m)\n",
    "\n",
    "m.save('1h.html')\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i) You should see points in the same cluster all over the map - briefly explain why that is. - (2.5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> A lot of the listings have similar amounts of reviews and prices for each respective entry. So it makes sense to put them in the same cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "j) How many clusters would you recommend using instead of 5? Display and interpret either the silhouette scores or the elbow method. - (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "temp = df.copy(deep=True)\n",
    "temp = temp[['longitude', 'latitude', 'price', 'number_of_reviews']]\n",
    "\n",
    "distortions = []\n",
    "K = range(2, 10)\n",
    "max_score = 0\n",
    "preferred_num_of_clusters = 0\n",
    "\n",
    "for k in K:\n",
    "    kmeans = KMeans(n_clusters=k).fit(temp)\n",
    "    distortions.append(kmeans.inertia_)\n",
    "    labels = kmeans.labels_\n",
    "    silhouette_score = metrics.silhouette_score(temp, labels)\n",
    "\n",
    "    if silhouette_score > max_score:\n",
    "        max_score = silhouette_score\n",
    "        preferred_num_of_clusters = k\n",
    "\n",
    "    print('Silhouette score for '  + str(k) + ' clusters is ' + str(silhouette_score))\n",
    "\n",
    "print('Silhouette score suggests that we use ' + str(preferred_num_of_clusters) + ' clusters')\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> We should probably use 2-3 clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k) Would you recommend normalizing the price and number of reviews? Briefly explain why. - (2.5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> It is always a good idea to normalise the features when they have different ranges, but not always the best thing to do. Normalisation will make sure that different features take on similar ranges and it can speed up the processing of the data.\n",
    "\n",
    "However, for K-Means it can be harmful, since it is a distance based clustering algorithm and therefore assumes that the normalised features have the same importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l) For all listings of type `Shared room`, plot the dendrogram of the hierarchical clustering generated from `longitude`, `latitude`, and `price`. - (5 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "temp = df.copy(deep=True)\n",
    "temp = temp[temp['room_type'] == 'Shared room']\n",
    "temp = temp[['latitude', 'longitude', 'price']]\n",
    "\n",
    "plt.figure(1, figsize=(20, 10))\n",
    "dendrogram(linkage(temp))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "m) briefly comment on what you observe from the structure of the dendrogram.  - (2.5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> There are listings in 2 major areas of the city. One of them has a large range of prices and the other one has a small range of prices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n) Normalize the `price` as in b) and repeat l) - (2.5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp['price'] = temp['price'].subtract(temp['price'].mean()).divide(temp['price'].std())\n",
    "temp = temp[['latitude', 'longitude', 'price']]\n",
    "\n",
    "plt.figure(1, figsize=(20, 10))\n",
    "dendrogram(linkage(temp))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 (50 points)\n",
    "\n",
    "This exercise will be using the [MNIST dataset](http://yann.lecun.com/exdb/mnist/).\n",
    "\n",
    "a) Using Kmeans, cluster the images using 10 clusters and plot the centroid of each cluster. - (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import load_digits\n",
    "import random\n",
    "import sys\n",
    "\n",
    "X, Y = load_digits(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x216 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=10)\n",
    "kmeans.fit(X)\n",
    "\n",
    "centroids = kmeans.cluster_centers_\n",
    "_, axes = plt.subplots(nrows=1, ncols=10, figsize=(10, 3))\n",
    "for ax, image in zip(axes, centroids):\n",
    "    ax.set_axis_off()\n",
    "    image = image.reshape(8, 8)\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\"nearest\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) what is the disagreement distance between the clustering you created above and the clustering created by the labels attached to each image? Briefly explain what this number means in this context. - (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The disagreement distance is: 98508\n"
     ]
    }
   ],
   "source": [
    "disagreement_distance = 0\n",
    "\n",
    "for i in range(len(Y)):\n",
    "    for j in range(i + 1, len(Y)):\n",
    "        disagreement_distance += (Y[i] == Y[j]) != (kmeans.labels_[i] == kmeans.labels_[j])\n",
    "\n",
    "print('The disagreement distance is: ' + str(disagreement_distance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "->answer\n",
    "\n",
    "For regrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Download the CIFAR-10 dataset [here](https://www.cs.toronto.edu/~kriz/cifar.html). Open `batch_1` by following the documentation on the web page. Plot a random image from the dataset. - (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='latin1')\n",
    "    return dict\n",
    "\n",
    "data_batch_1 = unpickle('data_batch_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand = random.randint(0, len(data_batch_1['data']))\n",
    "image = data_batch_1['data'][rand].reshape(3, 32, 32).transpose(1, 2, 0)\n",
    "plt.axis('off')\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) This image is 32 x 32 pixels and each pixel is a 3-dimensional object of RGB (Red, Green, Blue) intensities. Using the same image as in c), produce an image that only uses 4 colors (the 4 centroids of the clusters obtained by clustering the image itself using Kmeans). - (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_image_using_kmeans(original_image, k=4):\n",
    "    kmeans_for_K = KMeans(n_clusters=k).fit(original_image.reshape(\n",
    "        original_image.shape[0] * original_image.shape[1], 3)) \n",
    "\n",
    "    new_image = kmeans_for_K.cluster_centers_[kmeans_for_K.labels_]\n",
    "    new_image = np.clip(new_image.astype(int), 0, 255).reshape(\n",
    "        original_image.shape[0], original_image.shape[1], 3)\n",
    "    return new_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.axis('off')\n",
    "plt.imshow(compress_image_using_kmeans(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Write a function that applies this transformation to the entire dataset for any number K of colors. - (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_images(dataset, k=4):\n",
    "    compressed_images = []\n",
    "    i, n = 0, len(dataset) // 20\n",
    "    sys.stdout.write(\"\\r [%-20s] %d%% done\" % ('=' * (i // n), 5 * (i // n)))\n",
    "    \n",
    "    for image in dataset:\n",
    "        i += 1\n",
    "        image = image.reshape(3, 32, 32).transpose(1, 2, 0)\n",
    "        compressed_images.append(compress_image_using_kmeans(image, k))\n",
    "        if i % (len(dataset) // 20) == 0:\n",
    "            sys.stdout.write(\n",
    "                \"\\r [%-20s] %d%% done\" % ('=' * (i // n), 5 * (i // n)))\n",
    "\n",
    "    return compressed_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_images = compress_images(data_batch_1['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=10, figsize=(15, 5))\n",
    "fig.suptitle('Original to Compressed Images')\n",
    "\n",
    "for i, j in zip(np.random.randint(len(new_images), size=10), range(10)):\n",
    "    axes[0, j].set_axis_off()\n",
    "    axes[0, j].imshow(data_batch_1['data'][i].reshape(3, 32, 32).transpose(1, 2, 0))\n",
    "    axes[1, j].set_axis_off()\n",
    "    axes[1, j].imshow(new_images[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "82429e696b46c3c15b3b750c00f04c8ab1a33795e485b12d2eb3901d37b257c7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
