{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3 (100 points)\n",
    "\n",
    "The goal of this homework is to practice techniques relating to SVD.\n",
    "\n",
    "## Exercise 1 (65 points)\n",
    "\n",
    "a) Fetch the \"mnist_784\" data and store is as a `.csv` (that way you don't have to fetch it every time - which takes about 30s). (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from os.path import exists\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not exists('mnist.csv'):\n",
    "    X, y = fetch_openml(name=\"mnist_784\", version=1,\n",
    "                        return_X_y=True, as_frame=False)\n",
    "    pd.DataFrame({\n",
    "        'Image': X.tolist(),\n",
    "        'Label': y,\n",
    "    }).sort_values(by=['Label']).to_csv('mnist.csv', index=False)\n",
    "\n",
    "df = pd.read_csv('mnist.csv')\n",
    "df['Image'] = df['Image'].apply(lambda x: np.array(eval(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Plot the singular value plot for a single example of the 0 digit (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero = df[df['Label'] == 0].sample()\n",
    "zero = zero['Image'].iloc[0].reshape(28, 28)\n",
    "\n",
    "u, s, v = np.linalg.svd(zero)\n",
    "u.shape, s.shape, v.shape\n",
    "plt.plot(s)\n",
    "plt.xlabel('ith Singular Value')\n",
    "plt.ylabel('Singular Values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) By setting some singular values to 0, plot the approximation of the 0 digit next to the original digit. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = 6\n",
    "test = u[:, :rank] @ np.diag(s[:rank]) @ v[:rank]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2)\n",
    "axes[0].imshow(zero)\n",
    "axes[0].set_axis_off()\n",
    "axes[1].imshow(test)\n",
    "axes[1].set_axis_off()\n",
    "fig.suptitle('Original vs Compressed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Consider the entire dataset as a matrix. Perform SVD and store the dataset approximation in a new `.csv` file. Explain why / how you chose a particular rank. (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compressImage(image, rank=28):\n",
    "    u, s, v = np.linalg.svd(image.reshape(28, 28))\n",
    "    newImage = u[:, :(rank + 1)] @ np.diag(s[:(rank + 1)]) @ v[:(rank + 1)]\n",
    "    return newImage.reshape(784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_df = df.copy(deep=True)\n",
    "compressed_df['Image'] = df['Image'].apply(\n",
    "    lambda x: compressImage(x, 5).tolist())  # ???\n",
    "    # lambda x: compressImage(x, 5))  # ???\n",
    "\n",
    "compressed_df.to_csv('compressed_mnist.csv', index=False)\n",
    "\n",
    "# pd.DataFrame({\n",
    "#     # 'Image': compressed_df['Image'].tolist(), #???\n",
    "#     'Image': compressed_df['Image'], #??? \n",
    "#     'Label': compressed_df['Label'],\n",
    "# }).to_csv('compressed_mnist.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> answer\n",
    "One would probably look at the singular values on a graph, for each digit, and pick the number of singular values to keep, in order to have a relatively good reconstruction of each digit. I chose to keep 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) As in homework 2, using Kmeans on this new dataset, cluster the images from d) using 10 clusters and plot the centroid of each cluster. (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_df = pd.read_csv('compressed_mnist.csv')\n",
    "compressed_df['Image'] = compressed_df['Image'].apply(lambda x : np.array(eval(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = compressed_df.copy(deep=True)\n",
    "# data = data['Image']\n",
    "data = data['Image'].tolist()\n",
    "kmeans_compressed = KMeans(n_clusters=10).fit(data)\n",
    "\n",
    "_, axes = plt.subplots(1, 10)\n",
    "for ax, digit in zip(axes, kmeans_compressed.cluster_centers_):\n",
    "  ax.set_axis_off()\n",
    "  img = digit.reshape(28, 28)\n",
    "  ax.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) Repeat e) on the original dataset. Comment on any differences (or lack thereof) you observe. (8 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.copy(deep=True)\n",
    "# data = data['Image']\n",
    "data = data['Image'].tolist()\n",
    "kmeans = KMeans(n_clusters=10).fit(data)\n",
    "\n",
    "_, axes = plt.subplots(1, 10)\n",
    "for ax, digit in zip(axes, kmeans.cluster_centers_):\n",
    "  ax.set_axis_off()\n",
    "  img = digit.reshape(28, 28)\n",
    "  ax.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "->answer\n",
    "The kmeans centres with the original dataset seems to be clustering centres more poorly than the kmeans with the compressed dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g) Compare the disagreement distance of the clustering obtained in e) to the true labels, to the disagreement distance of the clustering obtained in f) to the true labels. Comment briefly. (8 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "def disagreement_dist(P_labels, C_labels):\n",
    "    \"\"\"\n",
    "    Calculate the disagreement distance between `P_Labels` and `C_Labels`\n",
    "    \"\"\"\n",
    "    ans = 0\n",
    "    n = 0\n",
    "    for i in range(len(P_labels)):\n",
    "        for j in range(i + 1, len(P_labels)):\n",
    "            n += 1\n",
    "            ans += (P_labels[i] == P_labels[j]) != (C_labels[i] == C_labels[j])\n",
    "            sys.stdout.write(\n",
    "                \"\\r %d\" % n)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_dist = disagreement_dist(kmeans_compressed.labels_, compressed_df['Label'].tolist())\n",
    "# original_dist = disagreement_dist(kmeans.labels_, df['Label'].tolist())\n",
    "# compressed_dist = disagreement_dist(kmeans_compressed.labels_, compressed_df['Label'])\n",
    "# original_dist = disagreement_dist(kmeans.labels_, df['Label'])\n",
    "\n",
    "print('The disagreement distance of the compressed image clustering is: {}'.format(compressed_dist))\n",
    "# print('The disagreement distance of the original image clustering is: {}'.format(original_dist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h) Create a matrix that is the difference between the original dataset and the rank-10 approximation of the dataset. (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_mat = np.zeros((784, 784))\n",
    "\n",
    "compressed_df = df.copy(deep=True)\n",
    "compressed_df['Image'] = compressed_df['Image'].apply(lambda x: compressImage(x, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i) The largest (using euclidean distance from the origin) rows of the matrix could be considered anomalous data points. Briefly explain why. Plot the 10 images responsible for the 10 largest rows. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 (35 points)\n",
    "\n",
    "a) Modify the code below to pick 4 categories of news articles that you think are minimally related (for example `sci.space` and `rec.sport.baseball`). (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "# from collections import defaultdict\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "\n",
    "categories = ['comp.graphics', 'rec.autos', 'sci.space', 'talk.politics.guns']\n",
    "news_data = fetch_20newsgroups(subset='train', categories=categories)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Using the `SnowballStemmer`, stem the words in every article (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "cleaned_articles = []\n",
    "stems = []\n",
    "\n",
    "for article in news_data['data']:\n",
    "    for token in word_tokenize(article):\n",
    "        stems.append(stemmer.stem(token))\n",
    "        # print(word)\n",
    "    # cleaned_articles.append(stems)\n",
    "    # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "872359\n"
     ]
    }
   ],
   "source": [
    "print(len(stems))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Use the `TfidfVectorizer` on the stemmed articles. Set `min_df` and `max_df` to reasonable numbers and briefly explain your reasoning. Store the resulting dataset into a `.csv` file. (7 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectoriser = TfidfVectorizer(max_df=1.0, min_df=1)\n",
    "X = vectoriser.fit_transform(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = pd.DataFrame.sparse.from_spmatrix(X)\n",
    "# file.to_csv('vectorised_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) For rank k ranging from 1 to 25:\n",
    "\n",
    "1. Reduce the dimensionality of the tfidf vectorized data using a dimension reduction technique discussed in class.\n",
    "2. Apply Kmeans on the reduced dataset to create 4 clusters\n",
    "3. Record the disagreement distance between the clustering in 2 and the article category\n",
    "\n",
    "Then plot the recorded disagreement distance per rank. Comment briefly. (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(872359, 31506)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduceDims(dataset, k):\n",
    "    u, s, v = np.linalg.svd(dataset)\n",
    "    return u[:, :(k + 1)] @ np.diag(s[:(k + 1)]) @ v[:(k + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = X.toarray()\n",
    "\n",
    "disagreement_distance = []\n",
    "for k in range(1,25):\n",
    "    # dim_reduced_dataset = # your code here\n",
    "    dim_reduced_dataset = reduceDims(test, k)\n",
    "    kmeans = KMeans(n_clusters=3, init='k-means++', max_iter=100, n_init=10, random_state=0)\n",
    "    kmeans.fit_predict(dim_reduced_dataset)\n",
    "    labelsk = kmeans.labels_\n",
    "    disagreement_distance.append(disagreement_dist(labelsk, news_data.target))\n",
    "    print(k)\n",
    "\n",
    "plt.plot(range(1,25), disagreement_distance)\n",
    "plt.ylabel('Disagreement')\n",
    "plt.xlabel('Dimension')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f2f0d8f4cb3aa975722cf0867e0489d9bb140832f0c7bc3ddf3c0be5b70ad06f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
