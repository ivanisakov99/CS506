{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3 (100 points)\n",
    "\n",
    "The goal of this homework is to practice techniques relating to SVD.\n",
    "\n",
    "## Exercise 1 (65 points)\n",
    "\n",
    "a) Fetch the \"mnist_784\" data and store is as a `.csv` (that way you don't have to fetch it every time - which takes about 30s). (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from os.path import exists\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not exists('mnist.csv'):\n",
    "    X, y = fetch_openml(name=\"mnist_784\", version=1,\n",
    "                        return_X_y=True, as_frame=False)\n",
    "    pd.DataFrame(X).join(pd.DataFrame({'Label': y})).to_csv('mnist.csv', index=False)\n",
    "\n",
    "df = pd.read_csv('mnist.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Plot the singular value plot for a single example of the 0 digit (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero = df[df['Label'] == 0].sample()\n",
    "zero = zero.iloc[:, :784].values.reshape(28, 28)\n",
    "\n",
    "u, s, v = np.linalg.svd(zero)\n",
    "plt.plot(s)\n",
    "plt.xlabel('ith Singular Value')\n",
    "plt.ylabel('Singular Values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) By setting some singular values to 0, plot the approximation of the 0 digit next to the original digit. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = 6\n",
    "compressed_zero = u[:, :rank] @ np.diag(s[:rank]) @ v[:rank]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2)\n",
    "axes[0].imshow(zero)\n",
    "axes[0].set_axis_off()\n",
    "axes[1].imshow(compressed_zero)\n",
    "axes[1].set_axis_off()\n",
    "fig.suptitle('Original vs Compressed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Consider the entire dataset as a matrix. Perform SVD and store the dataset approximation in a new `.csv` file. Explain why / how you chose a particular rank. (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compressImage(image, rank=28):\n",
    "    u, s, v = np.linalg.svd(image.reshape(28, 28))\n",
    "    newImage = u[:, :(rank + 1)] @ np.diag(s[:(rank + 1)]) @ v[:(rank + 1)]\n",
    "    return newImage.reshape(784)\n",
    "\n",
    "def compressData(data, rank=28):\n",
    "    compressedData = []\n",
    "    for row in data:\n",
    "        compressedData.append(compressImage(row, rank))\n",
    "    return np.array(compressedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "for i in range(10):\n",
    "    images.append(df[df['Label'] == i].sample())\n",
    "\n",
    "fig, axes = plt.subplots(1, 10)\n",
    "for ax, digit, img in zip(axes, range(10), images):\n",
    "    ax.set_xlabel(xlabel=str(digit))\n",
    "    _, s, _ = np.linalg.svd(img.iloc[:, :784].values.reshape(28, 28))\n",
    "    ax.plot(s)\n",
    "\n",
    "fig.set_figwidth(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not exists('compressed_mnist.csv'):\n",
    "    pd.DataFrame(compressData(df.iloc[:, :784].values, 9)).join(df['Label']).to_csv('compressed_mnist.csv', index=False)\n",
    "\n",
    "df_compressed = pd.read_csv('compressed_mnist.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compressed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One would probably look at the singular values on a graph, for each digit, and pick the number of singular values to keep, in order to have a relatively good reconstruction of each digit thus not losing too much information.\n",
    "I chose to compress to rank 9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) As in homework 2, using Kmeans on this new dataset, cluster the images from d) using 10 clusters and plot the centroid of each cluster. (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_compressed = KMeans(n_clusters=10).fit(df_compressed.iloc[:, :784].values)\n",
    "\n",
    "fig, axes = plt.subplots(1, 10)\n",
    "for ax, digit in zip(axes, kmeans_compressed.cluster_centers_):\n",
    "  ax.set_axis_off()\n",
    "  img = digit.reshape(28, 28)\n",
    "  ax.imshow(img)\n",
    "\n",
    "fig.set_figwidth(15)\n",
    "fig.set_figheight(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) Repeat e) on the original dataset. Comment on any differences (or lack thereof) you observe. (8 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=10).fit(df.iloc[:, :784].values)\n",
    "\n",
    "fig, axes = plt.subplots(1, 10)\n",
    "for ax, digit in zip(axes, kmeans.cluster_centers_):\n",
    "  ax.set_axis_off()\n",
    "  img = digit.reshape(28, 28)\n",
    "  ax.imshow(img)\n",
    "\n",
    "fig.set_figwidth(15)\n",
    "fig.set_figheight(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kmeans seems to be performing similarly across the compressed and the original datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g) Compare the disagreement distance of the clustering obtained in e) to the true labels, to the disagreement distance of the clustering obtained in f) to the true labels. Comment briefly. (8 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disagreement_dist(P_labels, C_labels):\n",
    "    \"\"\"\n",
    "    Calculate the disagreement distance between `P_Labels` and `C_Labels`\n",
    "    \"\"\"\n",
    "    ans = 0\n",
    "    num_of_classes = len(set(P_labels))\n",
    "    df_temp = pd.DataFrame({'Label': P_labels}).join(\n",
    "        pd.DataFrame({'Predicted Label': C_labels}))\n",
    "    \n",
    "    for i in range(num_of_classes):\n",
    "        temp = df_temp[df_temp['Label'] == i]\n",
    "        s1 = temp['Predicted Label'].value_counts()\n",
    "        nums = 0\n",
    "        sum = s1.sum()\n",
    "        for val in s1.values:\n",
    "            nums += val\n",
    "            ans += val * (sum - nums)\n",
    "\n",
    "        temp = df_temp[df_temp['Predicted Label'] == i]\n",
    "        s1 = temp['Label'].value_counts()\n",
    "        nums = 0\n",
    "        sum = s1.sum()\n",
    "        for val in s1.values:\n",
    "            nums += val\n",
    "            ans += val * (sum - nums)\n",
    "        \n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_dist = disagreement_dist(df_compressed['Label'].values, kmeans_compressed.labels_)\n",
    "original_dist = disagreement_dist(df['Label'].values, kmeans.labels_)\n",
    "\n",
    "print('The disagreement distance of the compressed image clustering is: {}'.format(compressed_dist))\n",
    "print('The disagreement distance of the original image clustering is: {}'.format(original_dist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distances seem to be similar. After running it a few times with different clusterings, they produce similar results. (They switch on being the better one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h) Create a matrix that is the difference between the original dataset and the rank-10 approximation of the dataset. (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compressed = pd.DataFrame(compressData(df.iloc[:, :784].values, rank=10)).join(\n",
    "    df[\"Label\"]\n",
    ")\n",
    "\n",
    "# ???\n",
    "\n",
    "diff_matrix = df.iloc[:, :784].values - df_compressed.iloc[:, :784].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i) The largest (using euclidean distance from the origin) rows of the matrix could be considered anomalous data points. Briefly explain why. Plot the 10 images responsible for the 10 largest rows. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcDistance(diff):\n",
    "    distances = []\n",
    "    for row in diff:\n",
    "        distances.append(np.linalg.norm(row))\n",
    "    return pd.Series(distances)\n",
    "\n",
    "df['Distance'] = calcDistance(diff_matrix)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by=['Distance'], ascending=False)\n",
    "\n",
    "fig, axes = plt.subplots(1, 10)\n",
    "for ax, digit in zip(axes, df.iloc[:, :784].values):\n",
    "  ax.set_axis_off()\n",
    "  img = digit.reshape(28, 28)\n",
    "  ax.imshow(img)\n",
    "\n",
    "fig.set_figwidth(15)\n",
    "fig.set_figheight(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These images seem to be anomalous because they are very deformed compared to the other images. They are either too thin or too thick and it is difficult to understand which digit is which."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 (35 points)\n",
    "\n",
    "a) Modify the code below to pick 4 categories of news articles that you think are minimally related (for example `sci.space` and `rec.sport.baseball`). (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ivan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/ivan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "categories = ['comp.graphics', 'rec.autos', 'sci.space', 'talk.politics.guns']\n",
    "news_data = fetch_20newsgroups(subset='train', categories=categories)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Using the `SnowballStemmer`, stem the words in every article (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english', ignore_stopwords=True)\n",
    "stemmed_articles = [' '.join(stemmer.stem(word) for sent in sent_tokenize(article) for word in word_tokenize(sent)) for article in news_data['data']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2317\n"
     ]
    }
   ],
   "source": [
    "print(len(stemmed_articles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Use the `TfidfVectorizer` on the stemmed articles. Set `min_df` and `max_df` to reasonable numbers and briefly explain your reasoning. Store the resulting dataset into a `.csv` file. (7 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectoriser = TfidfVectorizer(stop_words='english', min_df=1, max_df=1.0)\n",
    "\n",
    "vectorised_data = vectoriser.fit_transform(stemmed_articles)\n",
    "centred_data = vectorised_data - np.mean(vectorised_data, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(centred_data).join(pd.DataFrame({'Target' : news_data['target']})).to_csv('vectorised_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) For rank k ranging from 1 to 25:\n",
    "\n",
    "1. Reduce the dimensionality of the tfidf vectorized data using a dimension reduction technique discussed in class.\n",
    "2. Apply Kmeans on the reduced dataset to create 4 clusters\n",
    "3. Record the disagreement distance between the clustering in 2 and the article category\n",
    "\n",
    "Then plot the recorded disagreement distance per rank. Comment briefly. (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, s, v = np.linalg.svd(centred_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "disagreement_distance = []\n",
    "for k in range(1,25):\n",
    "    dim_reduced_dataset = u[:, :k] @ np.diag(s[:k]) @ v[:k]\n",
    "    kmeans = KMeans(n_clusters=3, init='k-means++', max_iter=100, n_init=10, random_state=0)\n",
    "    kmeans.fit_predict(dim_reduced_dataset)\n",
    "    labelsk = kmeans.labels_\n",
    "    disagreement_distance.append(disagreement_dist(labelsk, news_data.target))\n",
    "    sys.stdout.write('\\r {}'.format(k))\n",
    "\n",
    "plt.plot(range(1,25), disagreement_distance)\n",
    "plt.ylabel('Disagreement')\n",
    "plt.xlabel('Dimension')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f2f0d8f4cb3aa975722cf0867e0489d9bb140832f0c7bc3ddf3c0be5b70ad06f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
