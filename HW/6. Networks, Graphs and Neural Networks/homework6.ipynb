{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 6\n",
    "\n",
    "The focus of this homework will be Network and Graphs as well as Neural Networks.\n",
    "\n",
    "## Exercise 1 [25pts]\n",
    "\n",
    "In this exercise, you will try to recommend new collaborations to researchers\n",
    "of the Machine Learning community. Our approach will follow the guidelines of\n",
    "collaborative filtering: “If your past behavior/preferences were similar\n",
    "to some other user’s, your future behavior may be as well”. As an\n",
    "example, imagine you like Rolling Stones, Beatles and Jimmy Hendrix. It turns\n",
    "out that most people that like the aforementioned artists, are also fans of Eric\n",
    "Clapton. Then, it is very likely that if you listen to Eric Clapton’s music, you\n",
    "will like it as well.\n",
    "\n",
    "In this assignment you will implement a recommendation system for suggesting new collaborations to Machine Learning researchers.\n",
    "\n",
    "A network as a graph: A graph or network represents relationships among\n",
    "different entities (users of a social network, researchers, products, etc.). Those\n",
    "entities are represented as nodes and the relationships between them (friends\n",
    "on Facebook, co-authors of a research paper, products purchased together) as\n",
    "edges. When there is an edge between two nodes, x and y, we say that y is a\n",
    "neighbor (or friend) of x (and also - as the graphs we consider are undirected -\n",
    "x is also a neighbor of y).\n",
    "\n",
    "Each line of `edges.txt` contains the names of two researchers that have co-authored a paper in one of the top Machine Learning conferences (NeurIPS, ICLR, ICML) between 2010 and 2016.\n",
    "\n",
    "a) Write a function that reads the file “old edges.txt” and create a\n",
    "graph using NetworkX. (This is a tab-separated value (TSV) file, you may\n",
    "use packages such as Pandas to read it.) [5pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import pygraphviz as pgv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = pd.read_csv('edges.txt', names=['source', 'target'], sep='\\t')\n",
    "file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = nx.from_pandas_edgelist(file, source='source', target='target')\n",
    "person = 'A. David Edwards'\n",
    "print('Neighbours:', list(graph[person]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Recommend by number of common friends. [10pts]\n",
    "\n",
    "The intuition behind this recommendation algorithm is that if non-friend\n",
    "Y is your friend’s friend, then maybe Y should be your friend too. If\n",
    "person Y is the friend of many of your friends, then Y is an even better\n",
    "recommendation.\n",
    "\n",
    "Write a function `common_friends_number(G, X)` that, given G and an author\n",
    "X, returns a list of recommendations for X. The authors in this list are sorted\n",
    "by the number of common neighbors they have with X (and are not of course\n",
    "already friends with X). If there are ties, you can break them arbitrarily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_friends_number(G, X):\n",
    "    friends = G[X]\n",
    "    recommendations = Counter()\n",
    "\n",
    "    for friend in friends:\n",
    "        for recommendation in G[friend]:\n",
    "            if recommendation not in friends and recommendation != X:\n",
    "                recommendations[recommendation] += 1\n",
    "    \n",
    "    return sorted(recommendations, key=recommendations.get, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "friendRecommendations = common_friends_number(graph, person)\n",
    "print(friendRecommendations)\n",
    "print('\\nNumber of recommendations', len(friendRecommendations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Make recommendations using Jaccard’s Index. [10points]\n",
    "\n",
    "If `Γ(X)` is the set of neighbors of X, then the metric we used in part (c), assigns to a non-friend y, the following recommendation score (with respect to X):\n",
    "\n",
    "$$ \\text{score(y)} = |\\Gamma(X) \\cap \\Gamma(y)| $$\n",
    "\n",
    "Jaccard’s Index scales this score by taking into account the union of X and Y ’s neighbors. Intuitively, X and Y are more similar, if what they have in common is as close as possible to what they have together.\n",
    "\n",
    "Write a function `jaccard index(G, X)` that given G and an author X, returns a\n",
    "list of recommendations for X. The authors in this list are sorted by the number\n",
    "of their Jaccard Index with respect to X (and are not of course already friends\n",
    "with X). If there are ties, you can break them arbitrarily.\n",
    "\n",
    "$$ \\text{Jaccard Index} = \\frac{|\\Gamma(X) \\cap \\Gamma(y)|}{|\\Gamma(X) \\cup \\Gamma(y)|}  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_index(G, X):\n",
    "    friends = G[X]\n",
    "    recommendations = Counter()\n",
    "\n",
    "    for friend in friends:\n",
    "        for recommendation in G[friend]:\n",
    "            if recommendation not in friends and recommendation != X:\n",
    "                recommendations[recommendation] += 1\n",
    "\n",
    "    for rec in recommendations:\n",
    "        recommendations[rec] /= len(set(G[X]) | set(G[rec]))\n",
    "    \n",
    "    return sorted(recommendations, key=recommendations.get, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "friendRecommendations = jaccard_index(graph, person)\n",
    "print(friendRecommendations)\n",
    "print('\\nNumber of recommendations', len(friendRecommendations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 [55pts]\n",
    "\n",
    "This exercise will focus on Neural Networks and visualization.\n",
    "\n",
    "a) Write a function that takes a keras network and outputs an image (png format) of the network. [10pts]\n",
    "\n",
    "You can assume the model is sequential and only uses dense layers. The output image for\n",
    "\n",
    "```python\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(layers.Dense(2, input_dim=2))\n",
    "    model.add(layers.Dense(1))\n",
    "    model.compile(loss=\"binary_crossentropy\")\n",
    "```\n",
    "\n",
    "should look something like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename=\"example.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: use the networkx library (specifically the [to_agraph](https://networkx.org/documentation/stable/reference/generated/networkx.drawing.nx_agraph.to_agraph.html) method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualiseNetwork(network, filename = 'graph'):\n",
    "    G = nx.DiGraph(nodesep='1', ranksep='1')\n",
    "\n",
    "    # Iterate over the layers\n",
    "    for layer_index, layer in enumerate(network.layers):\n",
    "\n",
    "        for i in range(layer.input_shape[1]):\n",
    "            colour = ''\n",
    "            if layer_index == 0:    # Input Node\n",
    "                colour = \"#0000FF\"\n",
    "            else:   # Hidden Node\n",
    "                colour = \"#00FF00\"\n",
    "\n",
    "\n",
    "            G.add_node(str(layer_index) + ',' + str(i),\n",
    "                               shape=\"circle\",\n",
    "                            #    color=\"#00FF00\",\n",
    "                               color=colour,\n",
    "                               label='')\n",
    "\n",
    "\n",
    "            for j in range(layer.output_shape[1]):\n",
    "                if layer_index == len(network.layers) - 1:  # Output Node\n",
    "                    G.add_node(\n",
    "                        str(layer_index + 1) + ',' + str(j),\n",
    "                        shape=\"circle\",\n",
    "                        color=\"#0000FF\",\n",
    "                        label=''\n",
    "                    )\n",
    "                \n",
    "                # Connect the nodes between the layers\n",
    "                G.add_edge(\n",
    "                    str(layer_index) + ',' + str(i),\n",
    "                    str(layer_index + 1) + ',' + str(j),\n",
    "                    color=\"#FF0000\"\n",
    "                )\n",
    "\n",
    "    \n",
    "    img = nx.nx_agraph.to_agraph(G)\n",
    "    img.layout(prog='dot')\n",
    "    img.draw(filename + '.png')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import animation\n",
    "import random\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from PIL import Image as im\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.Dense(2, input_dim=2))\n",
    "model.add(layers.Dense(1))\n",
    "model.compile(loss=\"binary_crossentropy\")\n",
    "\n",
    "visualiseNetwork(model, '2a')\n",
    "Image(filename='2a.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Generate 100 datapoints of the form y = 3x + 1 + e where e ~ N(0, 1) and plot the data in a scatter plot [5pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.linspace(0, 10, 100)\n",
    "Y = np.array([3 * x + 1 + random.normalvariate(0, 1) for x in X])\n",
    "\n",
    "plt.scatter(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Create a Neural Network with no hidden layers (just input to ouput each with just one neuron), using the `mean_squared_error` loss and no activation function. Create an image of this model using a) then train this model on the dataset from b). In a 3D plot, plot the weight, the bias, and the loss value. [10pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(layers.Dense(units=1, input_dim=1))\n",
    "model.compile(loss='mean_squared_error')\n",
    "\n",
    "\n",
    "weights, bias = [], []\n",
    "get_weights = LambdaCallback(\n",
    "    on_epoch_end=lambda epoch, logs: weights.append(model.get_weights()[:-1]))\n",
    "get_bias = LambdaCallback(\n",
    "    on_epoch_end=lambda epoch, logs: bias.append(model.get_weights()[1]))\n",
    "\n",
    "history = model.fit(X,\n",
    "                Y,\n",
    "                batch_size=1,\n",
    "                epochs=200,\n",
    "                callbacks=[get_weights, get_bias],\n",
    "                verbose=0)\n",
    "\n",
    "loss = history.history['loss']\n",
    "weights = np.asarray(weights)[:, 0, 0, 0].tolist()\n",
    "bias = np.asarray(bias)[:, 0].tolist()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "ax.plot(weights, bias, loss)\n",
    "\n",
    "ax.set_xlim3d([min(weights), max(weights)])\n",
    "ax.set_xlabel('Weights')\n",
    "\n",
    "ax.set_ylim3d([min(bias), max(bias)])\n",
    "ax.set_ylabel('Bias')\n",
    "\n",
    "ax.set_zlim3d([min(loss), max(loss)])\n",
    "ax.set_zlabel('Loss')\n",
    "\n",
    "ax.view_init(15, -30)\n",
    "plt.show()\n",
    "visualiseNetwork(model, filename='2c')\n",
    "Image('2c.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Using matplotlib animation, re-train the model from c) and create an animation of the weight, bias, and loss at each training epoch. [10pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, bias = [], []\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.Dense(units=1, input_dim=1))\n",
    "model.compile(loss='mean_squared_error')\n",
    "\n",
    "weights, bias = [], []\n",
    "history = model.fit(X,\n",
    "                    Y,\n",
    "                    batch_size=1,\n",
    "                    epochs=200,\n",
    "                    callbacks=[get_weights, get_bias],\n",
    "                    verbose=0)\n",
    "\n",
    "loss = history.history['loss']\n",
    "weights = np.asarray(weights)[:, 0, 0, 0].tolist()\n",
    "bias = np.asarray(bias)[:, 0].tolist()\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_tight_layout(True)\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "\n",
    "def update(num):\n",
    "    line.set_data(data[:2, :num])\n",
    "    line.set_3d_properties(data[2, :num])\n",
    "    return line\n",
    "\n",
    "\n",
    "data = np.asarray([weights, bias, loss])\n",
    "line, = ax.plot(data[0, 0:1], data[1, 0:1], data[2, 0:1])\n",
    "\n",
    "ax.set_xlim3d([min(weights), max(weights)])\n",
    "ax.set_xlabel('Weights')\n",
    "\n",
    "ax.set_ylim3d([min(bias), max(bias)])\n",
    "ax.set_ylabel('Bias')\n",
    "\n",
    "ax.set_zlim3d([min(loss), max(loss)])\n",
    "ax.set_zlabel('Loss')\n",
    "\n",
    "ani = animation.FuncAnimation(fig, update, interval=20, blit=False)\n",
    "\n",
    "ani.save('2d.gif', writer='pillow')\n",
    "plt.close()\n",
    "Image('2d.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Generate data of the form y = 3x^3 + 2x^2 + x + 1 + e where e ~ N(0, 1) and plot the data in a scatter plot [5pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.linspace(-10, 10, 100)\n",
    "Y = np.array([\n",
    "    3 * (x**3) + 2 * (x**2) + x + 1 + random.normalvariate(0, 1) for x in X\n",
    "])\n",
    "\n",
    "plt.scatter(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) Create and train a neural network on the dataset from e) and plot the resulting curve through the scatter plot. (you can use any number of epochs, hidden layers etc.) Also create an image of the network using the function from a) [10pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(layers.Dense(units=8, activation='relu', input_dim=1))\n",
    "model.add(layers.Dense(units=8, activation='relu'))\n",
    "model.add(layers.Dense(units=8, activation='relu'))\n",
    "model.add(layers.Dense(units=1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "visualiseNetwork(model, '2f')\n",
    "\n",
    "model.fit(X, Y, epochs=400, verbose=0, batch_size=10)\n",
    "\n",
    "plt.scatter(X, model.predict(X))\n",
    "Image('2f.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g) Using matplotlib animation, create an animation of the resulting curve from your model at each training epoch (up to 100 epochs). [5pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper(x, model, filename):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(x, model.predict(x))\n",
    "    fig.savefig(filename + '.png')\n",
    "    plt.close()\n",
    "    return np.asarray(im.open(filename + '.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "epochs = 500\n",
    "filename = '2g'\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.Dense(units=8, activation='relu', input_dim=1))\n",
    "model.add(layers.Dense(units=8, activation='relu'))\n",
    "model.add(layers.Dense(units=8, activation='relu'))\n",
    "model.add(layers.Dense(units=1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "for _ in range(int(epochs / 10)):\n",
    "    model.fit(X, Y, epochs=10, verbose=0, batch_size=10)\n",
    "    images.append(im.fromarray(helper(X, model, filename)))\n",
    "\n",
    "images[0].save(\n",
    "    filename + '.gif',\n",
    "    optimize=False,\n",
    "    save_all=True,\n",
    "    append_images=images[1:],\n",
    "    loop=0,\n",
    "    duration=300\n",
    ")\n",
    "\n",
    "Image(filename + '.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 [20pts]\n",
    "\n",
    "This exercise will focus on Neural Networks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ivan/opt/anaconda3/envs/hw6/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torchvision \n",
    "import torchvision.transforms as transforms \n",
    "from torch.optim import SGD, Adam\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Modify the number of layers to include atleast 2 hidden layers with appropriate number of neurons that use the sigmoid function in the forward pass: [5pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        # You are free to define any number of layers you want to here\n",
    "\n",
    "        # self.layer1 = nn.Linear(input_size, 800)\n",
    "\n",
    "        # your code here\n",
    "        # self.layer2 = nn.Linear(800, 400)\n",
    "        # # self.layer3 = nn.Linear(512, 256)\n",
    "        # self.layer4 = nn.Linear(400, 64)\n",
    "        # self.output = nn.Linear(64, 10)\n",
    "\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        # self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # x = F.relu(self.layer1(input))\n",
    "\n",
    "        # your code here\n",
    "        # x = self.sigmoid(self.layer2(x))\n",
    "        # # x = self.sigmoid(self.layer3(x))\n",
    "        # x = self.sigmoid(self.layer4(x))\n",
    "\n",
    "        # Accuracy 88\n",
    "        # x = self.dropout(F.relu(self.fc1(input)))\n",
    "        # x = self.dropout(F.relu(self.fc2(x)))\n",
    "        # x = self.dropout(F.relu(self.fc3(x)))\n",
    "        # return F.log_softmax(self.fc4(x), dim=1)\n",
    "        # x = F.relu(self.layer1(input))\n",
    "        x = self.dropout(F.relu(self.fc1(input)))\n",
    "        x = self.dropout(self.sigmoid(self.fc2(x)))\n",
    "        x = self.dropout(self.sigmoid(self.fc3(x)))\n",
    "        return F.log_softmax(self.fc4(x), dim=1)\n",
    "\n",
    "        # outputs = self.output(x)\n",
    "\n",
    "        # print(outputs.shape)\n",
    "        # return F.softmax(outputs)\n",
    "        # return F.softmax(self.fc4(x), dim=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Modify the following values to provide the highest accuracy on your Neural Network [2pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "epochs = 10                      # number of epochs\n",
    "# epochs = 20\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")     # device\n",
    "print(device)\n",
    "criterion = nn.CrossEntropyLoss() # loss function\n",
    "# criterion = nn.NLLLoss()\n",
    "batch_size = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell everytime you update parts a and b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NeuralNetwork(784).to(device)\n",
    "optimizer = Adam(net.parameters(), lr = 1e-2)\n",
    "\n",
    "total_loss = 0\n",
    "\n",
    "\n",
    "#######################################\n",
    "### Downloading the data\n",
    "\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(training_data, batch_size = batch_size, shuffle = True, num_workers=2, drop_last=True)\n",
    "train_dataloader2 = torch.utils.data.DataLoader(training_data, batch_size = batch_size, shuffle = True, num_workers=2, drop_last=True)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data, batch_size = batch_size, shuffle = True, num_workers=2, drop_last=True)\n",
    "test_dataloader2 = torch.utils.data.DataLoader(test_data,\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=True,\n",
    "                                               num_workers=2,\n",
    "                                               drop_last=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) You may change the model to increase the accuracy of this model. The goal is to attain the highest possible accuracy. You do not get marks for accuracies less than 83%. You may modify the values in parts a and b [8pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:07<01:05,  7.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " — epoch loss = 108.95 — training accuracy = 0.83 — test accuracy = 0.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:14<00:57,  7.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " — epoch loss = 49.90 — training accuracy = 0.87 — test accuracy = 0.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:21<00:49,  7.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " — epoch loss = 42.35 — training accuracy = 0.86 — test accuracy = 0.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:28<00:41,  6.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " — epoch loss = 38.46 — training accuracy = 0.89 — test accuracy = 0.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:35<00:34,  6.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " — epoch loss = 37.18 — training accuracy = 0.89 — test accuracy = 0.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:41<00:27,  6.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " — epoch loss = 34.64 — training accuracy = 0.90 — test accuracy = 0.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:48<00:20,  6.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " — epoch loss = 33.30 — training accuracy = 0.90 — test accuracy = 0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:55<00:13,  6.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " — epoch loss = 32.37 — training accuracy = 0.91 — test accuracy = 0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [01:02<00:06,  6.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " — epoch loss = 30.89 — training accuracy = 0.90 — test accuracy = 0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:09<00:00,  6.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " — epoch loss = 30.23 — training accuracy = 0.91 — test accuracy = 0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(epochs)):\n",
    "    epoch_loss = 0\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        inputs, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs.view(batch_size,-1).to(device))\n",
    "\n",
    "        labels_ = F.one_hot(labels, num_classes= 10)\n",
    "\n",
    "        loss = criterion(outputs,labels_.to(device).float())\n",
    "        loss.backward()  # update network parameters\n",
    "\n",
    "        optimizer.step() # update the optimizer parameters\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, data in enumerate(train_dataloader2):\n",
    "            input, labels = data\n",
    "            outputs = net(input.view(batch_size,-1).to(device))\n",
    "\n",
    "            total+= len(labels)\n",
    "\n",
    "            predictions = torch.argmax(outputs, dim = 1)\n",
    "            predictions = predictions.to(\"cpu\").numpy()\n",
    "            correct += sum(1*(labels.numpy()==predictions))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        correct_test = 0\n",
    "        total_test = 0\n",
    "        for i, data in enumerate(test_dataloader):\n",
    "            input, labels = data\n",
    "            outputs = net(input.view(batch_size,-1).to(device))\n",
    "\n",
    "            total_test+= len(labels)\n",
    "\n",
    "            predictions = torch.argmax(outputs, dim = 1)\n",
    "            predictions = predictions.to(\"cpu\").numpy()\n",
    "            correct_test += sum(1*(labels.numpy()==predictions))\n",
    "\n",
    "    print(\n",
    "        \" — epoch loss = %1.2f — training accuracy = %1.2f — test accuracy = %1.2f\"\n",
    "        % (epoch_loss, correct / total, correct_test / total_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Explain in ~150 words the method you used to increase the accuracy. [5pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
